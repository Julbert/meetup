{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Churn prediction MeetUp hands-on",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "14efR2pwxTW5PAzX2qVuu7QQ9H-iuaGkd",
          "timestamp": 1528119580862
        },
        {
          "file_id": "1PpN4irJNmpS6cbzY3C3h7CywmQ2Dxniu",
          "timestamp": 1528033888522
        }
      ],
      "collapsed_sections": [
        "pvAqrKyeYBBI",
        "q5gh0MhbYBB7",
        "8nYyceiFYBD9",
        "KHuFcFlNYBEQ",
        "Pt9Rgb_uYBEm",
        "w8QuMnX9YBFv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Y0xih3PWYBBD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hands-on: churn classification on Ziggo subscriptions\n",
        "\n",
        "In this notebook you will build several churn prediction models, each more complex and advanced than the previous.\n",
        "We have facilitated in data preparation and cleaning so you don't have to.\n",
        "\n",
        "The notebook contains several exercises. If you have any questions, just raise your hand and we'll help out shortly.\n",
        "\n",
        "\n",
        "**The first exercise is in chapter 5. The first 4 give you an overview of the approach, but you don't have to do anything yourself.**\n",
        "\n",
        "Don't spend too much time in trying to understand the fine print of the code, but try to keep the big picture. You can read the notebook later at home.\n",
        "\n",
        "As we don't know how Google will respond to 50 sessions from the same IP-address, please be patient. ;)\n",
        "\n",
        "**Disclaimer from BigData Republic: while you are free to copy this notebook for your own personal use, we ask that you be considerate and remove any remaining data left on your system. Thank you.**"
      ]
    },
    {
      "metadata": {
        "id": "Eb32ppcmFCiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Table of contents\n",
        "\n",
        "\n",
        "1.   Setup\n",
        "2.   Loading the data\n",
        "3.   Data Exploration\n",
        "4.   Prepare for general ML\n",
        "5.   Vanilla Pipeline\n",
        "6.   Recurrent Neural Network\n",
        "7.   Parameter grid search\n",
        "8.   Evaluating on the test set\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pvAqrKyeYBBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1 - Setup\n",
        "Google Drive authentication and imports."
      ]
    },
    {
      "metadata": {
        "id": "kznkDdpgZxRa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The cell below will prompt you to authenticate yourself to Google through a link. This is for accessing the dataset from Google Drive.\n",
        "Google Colaboratory is a new and experimental service by Google. This is why it does not yet have default access to your other Google attributes.\n"
      ]
    },
    {
      "metadata": {
        "id": "sbjBvsU7Y-w7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive ## you will have install for every colab session\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Here we define the directory holding the dataset, this folder should contain the parquet file.\n",
        "link_id = \"18umCPUfyMUnypV094Q3MK6X6fX-2VrmN\"\n",
        "drive_file = drive.CreateFile({'id': link_id})\n",
        "datafile = 'churn_meetup_dataset.parquet'\n",
        "drive_file.GetContentFile(datafile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIf1x87iKdqq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lastly, we install and import dependencies."
      ]
    },
    {
      "metadata": {
        "id": "jkciuvBzHOUK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "# this looks weird, but we want pandas-0.23\n",
        "!pip uninstall -y pandas\n",
        "!pip install pandas\n",
        "\n",
        "!pip install git+https://github.com/hyperopt/hyperopt\n",
        "!pip install cython pyarrow pandas_profiling\n",
        "!pip install git+https://github.com/BigDataRepublic/bdr-analytics-py.git\n",
        "  \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "matplotlib.style.use('ggplot')\n",
        "#%load_ext autoreload\n",
        "#%autoreload 1\n",
        "#%aimport bdranalytics\n",
        "#import bdranalytics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "import seaborn as sns\n",
        "from scipy.ndimage.interpolation import shift\n",
        "import sklearn\n",
        "from sklearn import linear_model, model_selection\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "from sklearn.feature_selection import RFE, RFECV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import itertools\n",
        "from sklearn.metrics import make_scorer, r2_score\n",
        "from sklearn.metrics.scorer import r2_scorer, mean_squared_error_scorer\n",
        "import statsmodels\n",
        "import statsmodels.tsa.api as sm\n",
        "from IPython.display import display\n",
        "import IPython\n",
        "print(\"IPython version: {}\".format(IPython.__version__))\n",
        "print (\"numpy: {}\".format(np.__version__))\n",
        "print (\"scipy: {}\".format(sc.__version__))\n",
        "print (\"sklearn: {}\".format(sklearn.__version__))\n",
        "print (\"pandas: {}\".format(pd.__version__))\n",
        "import keras\n",
        "print (\"keras: {}\".format(keras.__version__))\n",
        "import tensorflow as tf\n",
        "print (\"tensorflow: {}\".format(tf.__version__))\n",
        "print (\"Imports completed.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NumyReOKYBFS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setting up Keras\n",
        "For learning neural networks, we'll be using keras. And Luckily Google Colaboratory has a simple GPU available for faster learning."
      ]
    },
    {
      "metadata": {
        "id": "G1RBkaJrYBFU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Some necessary imports\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, TimeDistributed, Dropout\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import losses\n",
        "\n",
        "import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvPxyfrqETKe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Verify that the GPU is indeed available."
      ]
    },
    {
      "metadata": {
        "id": "Bk88SWjD9TwX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VRuNSaZYYBFV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In case of non-GPU devices, we could enable multithreaded CPU to speed training up a bit"
      ]
    },
    {
      "metadata": {
        "id": "M_KMXVV-YBFW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if False:  # Run optional if no GPU available\n",
        "  import tensorflow as tf\n",
        "  config = tf.ConfigProto(\n",
        "      intra_op_parallelism_threads=1, \n",
        "      inter_op_parallelism_threads=1,\n",
        "      allow_soft_placement=True, \n",
        "      device_count = {'CPU': 8}, \n",
        "      log_device_placement=True)\n",
        "  session = tf.Session(config=config)\n",
        "  K.set_session(session)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q5gh0MhbYBB7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2 - Loading data"
      ]
    },
    {
      "metadata": {
        "id": "TNVaKUjHSSbo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the dataset:\n",
        "Xy_orig = pd.read_parquet(datafile).sort_values(by=['customer_uid', 'month']).reset_index(drop=True)\n",
        "Xy_orig.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j80tHpzYYBCu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the target variable\n",
        "In the data set we've just loaded, the column \"is_churned\" contains the information we are aiming to predict.\n",
        "Each row shows the information available about the user at a given timestamp and whether he has churned or not.\n",
        "\n",
        "In practice, we don't want to know if a customer is churned now (*current* state), but if he will churn in the future (the *next* state). Therefore we shift the `is_churned` column by `-1` to get the value of 1 time ahead.\n",
        "\n",
        "Note that this will result in a NaN value on the last index, which we therefore remove."
      ]
    },
    {
      "metadata": {
        "id": "oV1ruZ1ZYBCv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def initially_churned_rows(df):\n",
        "    i=0\n",
        "    while i < len(df) and df['is_churned'].iloc[i]==1:\n",
        "        i+=1\n",
        "    return i\n",
        "\n",
        "def prepare_user(df):\n",
        "    df = df.sort_values(by=['customer_uid', 'month'])\n",
        "    df['initially_churned'] = 0\n",
        "    df['initially_churned'][0:(initially_churned_rows(df))] = 1\n",
        "    out_x = df.drop('is_churned', axis=1)\n",
        "    out_y = df['is_churned'].shift(-1).dropna()\n",
        "    out = pd.concat([\n",
        "        out_x,  # the features\n",
        "        out_y.to_frame() # if is churned the next month\n",
        "        ], axis=1, join_axes=[df.index]).loc[out_y.index,:]\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tft09QPmYBC0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "Xy = Xy_orig.groupby(by=['customer_uid']).apply(prepare_user)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16Poz3aFYBC6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Selecting a hold out set"
      ]
    },
    {
      "metadata": {
        "id": "r8Um1zOzYBC8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we want to split the data set into training, test and validation sets.\n",
        "In this step, we need to take into account that multiple rows of the data set refer to the same customer (1 row per timestamp).\n",
        "We want to make sure that customers are split into trainining and test set, i.e. all the rows correspoding to one given customer must be in only one of the sets. That is important in order to prevent information leakage, which could give us an overestimate of the model's performance.\n",
        "\n",
        "We also want to make sure that ever set has the same proportion of churned and not-churned customers, so we will use a stratified split."
      ]
    },
    {
      "metadata": {
        "id": "VV3T9yUDYBC-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Get all user ids:\n",
        "users = Xy_orig[['customer_uid', 'is_churned']].groupby(by=['customer_uid']).max().reset_index()\n",
        "\n",
        "# and split users into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "users_train, users_testing, _, _ = train_test_split(users, users['is_churned'], test_size=0.33, random_state=42, stratify=users['is_churned'])\n",
        "users_valid, users_test, _, _ = train_test_split(users_testing, users_testing['is_churned'], test_size=0.5, random_state=42, stratify=users_testing['is_churned'])\n",
        "\n",
        "train_users = users_train['customer_uid'].values\n",
        "validation_users = users_valid['customer_uid'].values\n",
        "holdout_users = users_test['customer_uid'].values\n",
        "\n",
        "print('{} users in the training set'.format(len(train_users)))\n",
        "print('{} users in the validation set'.format(len(validation_users)))\n",
        "print('{} users in the hould out/test set'.format(len(holdout_users)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TkNgXB9YYBDs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Use each set of users to split the data into those same sets:\n",
        "\n",
        "Xy = Xy.set_index(['customer_uid','month'])\n",
        "Xy_train = Xy[Xy.index.get_level_values('customer_uid').isin(train_users)]\n",
        "Xy_valid = Xy[Xy.index.get_level_values('customer_uid').isin(validation_users)]\n",
        "Xy_test = Xy[Xy.index.get_level_values('customer_uid').isin(holdout_users)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZnBEF1twYBD1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Finally, define X (containing only features) and y (only labels):\n",
        "def split_X_y(df):\n",
        "    return df.drop('is_churned', axis=1), df['is_churned']\n",
        "\n",
        "X_train, y_train = split_X_y(Xy_train)\n",
        "X_valid, y_valid = split_X_y(Xy_valid)\n",
        "X_test, y_test = split_X_y(Xy_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6z2zbDnszqGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One of the features is called `initially_churned`, which reflects the initial churn state of the customer. That information should not be one of the features used in the model. We'll store them separately for later use."
      ]
    },
    {
      "metadata": {
        "id": "14I7hQYGYBEt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_init_churn = X_train['initially_churned']\n",
        "X_train.drop(['initially_churned'], axis=1, inplace=True)\n",
        "\n",
        "valid_init_churn = X_valid['initially_churned']\n",
        "X_valid.drop(['initially_churned'], axis=1, inplace=True)\n",
        "\n",
        "test_init_churn = X_test['initially_churned']\n",
        "X_test.drop(['initially_churned'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O4juVomLW_wy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset is now split and indexed by customer id and month. For each customer we have a history of 2 years of records, corresponding to the month number ranging from 0 to 23."
      ]
    },
    {
      "metadata": {
        "id": "8nYyceiFYBD9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Data exploration\n",
        "An essential part of the modelling task is understanding the data we are working with.\n",
        "For classification, for example, is very important to check the balance between positive (churned) and negative (non-churned) samples.\n",
        "Other important analysis are proportion of missing values, correlations between pairs of features and between each feature and the label etc."
      ]
    },
    {
      "metadata": {
        "id": "DNOL_J1KYBD-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Number of churned and non-churned users:\n",
        "print(\"Total nr of users:              {}\".format(len(Xy.index.get_level_values('customer_uid').unique())))\n",
        "print(\"Nr of churned users in dataset: {}\".format(len(Xy.index.get_level_values('customer_uid')[Xy['is_churned']==1].unique())))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h36bC_STEcfE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "Xy.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSqDp3lFC-cs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Missing values per feature:\n",
        "def count_missing_values(col):\n",
        "  print('Feature {}: {} %missing values'.format(col.name, col.isnull().sum()/len(col)*100))\n",
        "  \n",
        "_= Xy.apply(lambda col: count_missing_values(col), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V9H5iSGY0-Pu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...\n",
        "\n",
        "Surprise! We already cleaned the dataset for you. No missing values here."
      ]
    },
    {
      "metadata": {
        "id": "KHuFcFlNYBEQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Prepare dataset for general ML\n",
        "Most estimators cannot handle categorical features. Therefore, those features have to be first encoded using strategies such as one hot encoding, weight of evidence or leave one out.\n",
        "In order to do so, we first separate all features according to their type.\n",
        "\n",
        "Regarding categorical features, column names have been removed and so the number of unique values is essential to determine the most appropriate encoding strategy.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "qQQLR1rcYA-u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot to get an idea of the data cardinality\n",
        "cardinality = X_train.apply(lambda s: s.nunique()).sort_values(ascending=False)\n",
        "cardinality.plot(kind='bar', figsize=(30,3))\n",
        "plt.ylim((0,100))\n",
        "plt.title(\"Number of unique values per feature\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TPMlX2vNZtKN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All features have been either encoded, dummified or normalized. In fact, most features are categorical and have less than 20 unique values. The remaining few are numerical columns.\n",
        "\n",
        "It is a given that all features with less than 50 unique values are categoricals."
      ]
    },
    {
      "metadata": {
        "id": "nveREO2_bR7I",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "categoricals = cardinality[cardinality < 50].index\n",
        "numericals = cardinality[cardinality >= 50].index\n",
        "\n",
        "print(\"Numericals: {} features\".format(len(numericals)))\n",
        "print(\"Categoricals: {} features\".format(len(categoricals)))\n",
        "print(\"Data types:\")\n",
        "X_train.dtypes.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMTn04WYdHad",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "No categorical is higher cardinal than 40, meaning that for a first version we can probably make do with One Hot Encoding."
      ]
    },
    {
      "metadata": {
        "id": "pOULObkLIPfF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 - Vanilla pipeline\n",
        "We will start by setting up a very simple model pipeline, so that the main required steps become clear.\n",
        "That means that the way each step is performed is neither ideal nor clean coded, our only goal here is to make the process understandable.\n",
        "\n",
        "Once you understand how each step works and the purpose of it, you can move to the advanced pipeline and also implement your own solution to try to improve performance.\n",
        "\n",
        "The vanilla pipeline will include:\n",
        "* Categorical encoding using OHE only (\"highly categoricals\" will be ignored)\n",
        "* Feature scaling\n",
        "* implementation of a sklearn classifier\n",
        "\n",
        "The vanilla pipeline does not include:\n",
        "* Cross-validation + hyperparameter tunning of models \n",
        "* Encoding of  \"highly categoricals\" with e.g. Weight of Evidence\n",
        "* Solution for class imbalance"
      ]
    },
    {
      "metadata": {
        "id": "JOJLD-OgrcsW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoders and scalers\n",
        "First we train encoders and scalers."
      ]
    },
    {
      "metadata": {
        "id": "h8xpk1LrJ8m4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Fit encoders in the training set\n",
        "\n",
        "# encoders\n",
        "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "my_scaler = StandardScaler()\n",
        "\n",
        "# OHE cannot deal with strings, so make sure to encode strings to numbers first.\n",
        "# Luckily, we've already done that.\n",
        "\n",
        "# Join OHE output and numerical features.\n",
        "X_train_ohe = ohe.fit_transform(X_train[categoricals])\n",
        "X_train_pp = np.hstack([X_train[numericals].values, X_train_ohe])\n",
        "\n",
        "# Scale features to be standard mean and variance.\n",
        "X_train_pp = my_scaler.fit_transform(X_train_pp)\n",
        "\n",
        "print('{} cols before encoding'.format(X_train.shape[1]))\n",
        "print('{} cols after encoding'.format(X_train_pp.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ml0vViYGaIhv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**: also train a variance selector to remove features that don't have much information.\n",
        "\n",
        "*Advanced*: features are time bound and user bound. Try to think of ways of checking variance in these groups."
      ]
    },
    {
      "metadata": {
        "id": "ubJ2BawGaXpe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer**:"
      ]
    },
    {
      "metadata": {
        "id": "z26ibHAraEoV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# thresholder = VarianceThreshold(...)\n",
        "# thresholder.fit(...)\n",
        "# x_train_pp = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fapNg5PmtQWt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We then apply these learned encoders and scalers to the hold-out sets."
      ]
    },
    {
      "metadata": {
        "id": "cHGRPDUmOTz2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "X_valid_ohe = ohe.transform(X_valid[categoricals])\n",
        "X_valid_pp = np.hstack([X_valid[numericals].values, X_valid_ohe])\n",
        "X_valid_pp = my_scaler.transform(X_valid_pp)\n",
        "\n",
        "# Holdout\n",
        "X_test_ohe = ohe.transform(X_test[categoricals])\n",
        "X_test_pp = np.hstack([X_test[numericals].values, X_test_ohe])\n",
        "X_test_pp = my_scaler.transform(X_test_pp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XH00BoqaMut6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Pointer**: Try to refrain from looking at the test set until the end of the notebook :-). You don't have access to the test set in real life either."
      ]
    },
    {
      "metadata": {
        "id": "c5LsdSiVapke",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise** if you did the execise with the variance thresholder, also apply it to the validation and test set."
      ]
    },
    {
      "metadata": {
        "id": "IDnsuJ1pazDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ]
    },
    {
      "metadata": {
        "id": "JjT4Mn0-ao6U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# x_valid_pp = ...\n",
        "# x_test_pp = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMwHkBSxr5Jn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Fitting a baseline\n",
        "\n",
        "**Exercise:** train a linear regression model on the resulting train set.\n",
        "Now we fit a simple linear regression model on the resulting dataset.\n"
      ]
    },
    {
      "metadata": {
        "id": "8HgWLNMFRr9q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ]
    },
    {
      "metadata": {
        "id": "7Z05_js-R1ki",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# lr = ...   # Logistic regression model\n",
        "# y_pred_val_lr = ...  # Thresholded prediction\n",
        "# y_pred_proba_val_lr = ...  # Probability prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BZ0wct6Mtvn4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluating the baseline\n",
        "Evaluate our trained model on our hold-out set. Let's try the sklearn classification report and AUC metrics first.\n"
      ]
    },
    {
      "metadata": {
        "id": "R1pfA0bAOT30",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, auc, roc_curve, confusion_matrix\n",
        "\n",
        "def plot_auc(y, pred_proba, pred):\n",
        "  \n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.subplot(1,2,1)\n",
        "  fpr, tpr, thresholds = roc_curve(y, pred_proba[:,1])\n",
        "  \n",
        "  lw = 2\n",
        "  plt.plot(fpr, tpr, color='darkorange',\n",
        "           lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.title('ROC Curve')\n",
        "  \n",
        "  plt.subplot(1,2,2)\n",
        "  sns.heatmap(confusion_matrix(y, pred), annot=True)\n",
        "  plt.title('Confusion Matrix')\n",
        "  \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkjkXZiNRUoO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('Classification report on the validaion set:')\n",
        "print(classification_report(y_valid, y_pred_val_lr))\n",
        "plot_auc(y_valid, y_pred_proba_val_lr, y_pred_val_lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J5d1g1HYSG-W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise**: why are the accuracy and AUC so high? What other metric could we have used instead?"
      ]
    },
    {
      "metadata": {
        "id": "c8-qj3GPvgmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer: ** ...\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "hiMGk2-NbPf8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you don't want to think about the answer, below is an alternative evaluation plot that might provide us with more understanding."
      ]
    },
    {
      "metadata": {
        "id": "3a2gsNf_vSwP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, accuracy_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def summary_print(y_true, y_score, dataset='Avg'):\n",
        "    average_precision = average_precision_score(y_true, y_score)\n",
        "    print('{0: <10} Accuracy score: {1:0.2f}'.format(dataset, accuracy_score(y_true, [round(x) for x in y_score])))\n",
        "    print('{0: <10} Average precision-recall score: {1:0.2f}'.format(dataset,\n",
        "          average_precision))\n",
        "    \n",
        "def summary_plot(y_true, y_score, dataset='Avg'):\n",
        "    average_precision = average_precision_score(y_true, y_score)\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
        "    plt.step(recall, precision, color='b', alpha=0.2,\n",
        "             where='post')\n",
        "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                     color='b')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    _=plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
        "    \n",
        "def model_summary(y_true, y_score, dataset='Avg'):\n",
        "    summary_print(y_true, y_score, dataset)\n",
        "    summary_plot(y_true, y_score, dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eW-mSKJcwL61",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_summary(y_valid, y_pred_proba_val_lr[:,1], 'Validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zoIdb-OIxJoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, we still have some room for improvement as our model, as the decision landscape looks strange. We also did not account for temporal patterns in the dataset, and we assumed records to be independent, even though they could potentially belong to the same customer."
      ]
    },
    {
      "metadata": {
        "id": "HbXGUWb1SxBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Optional Exercise:** Train and evaluate a random forest model.\n",
        "What other improvements does it bring?"
      ]
    },
    {
      "metadata": {
        "id": "isCp3SndS-m6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answer:**"
      ]
    },
    {
      "metadata": {
        "id": "hFX6zl9NSq6r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DSucVoX7xXkn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6 - A simple recurrent neural network\n",
        "We'll build a better model now, in the form of a neural network. Neural networks are an ideal model class for this hands-on as you know zero to nothing about features and domain understanding is not an option. Feature engineering is baked in neural nets.\n",
        "\n",
        "Another viable option would be to use different types of tree based models, but those are difficult to modify to account for temporal patterns."
      ]
    },
    {
      "metadata": {
        "id": "C5lxKSiJ6dXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reshape dataset for neural networks\n",
        "Neural networks need a different type of preprocessing. Besides normal networks, we're also dealing with temporal patterns to account for.\n",
        "\n",
        "In order for the network to understand temporal patterns, we have to rearrange the dataset to include a time layer per customer. For time series models, the end result should be a tensor of shape  `samples * timesteps * features`."
      ]
    },
    {
      "metadata": {
        "id": "H2xzTjMB6dX3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "num_series_train = X_train.index.get_level_values(0).nunique()\n",
        "num_series_valid = X_valid.index.get_level_values(0).nunique()\n",
        "num_series_test = X_test.index.get_level_values(0).nunique()  \n",
        "data_dim = X_train_pp.shape[1]\n",
        "\n",
        "# Using reshape assumes that the data has been ordered in the right way.\n",
        "# In this case, the data should be and is already sorted first by customer id\n",
        "# and then by measurement month.\n",
        "X_train_t = X_train_pp.reshape(num_series_train, -1, data_dim)\n",
        "X_valid_t = X_valid_pp.reshape(num_series_valid, -1, data_dim)\n",
        "X_test_t = X_test_pp.reshape(num_series_test, -1, data_dim)\n",
        "\n",
        "# Same treatment should be given to the labels\n",
        "y_train_t = y_train.values.reshape(num_series_train, -1, 1)\n",
        "y_valid_t = y_valid.values.reshape(num_series_valid, -1, 1)\n",
        "y_test_t = y_test.values.reshape(num_series_test, -1, 1)\n",
        "\n",
        "assert X_train_t.shape[0] == y_train_t.shape[0]\n",
        "assert X_train_t.shape[1] == y_train_t.shape[1]\n",
        "\n",
        "timesteps = X_train_t.shape[1]\n",
        "\n",
        "display(\"Train set shape\", X_train_t.shape)\n",
        "display(\"Train label shape\", y_train_t.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pt9Rgb_uYBEm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optional: Coping with class imbalances\n",
        "It's always important to see what class balance we actually have. Most ML algorithms minimize some average error/loss. To put more weight on the minority class, we can give them inverse weights, causing the model to minimize average class error.\n",
        "\n",
        "We feed these weights into the model."
      ]
    },
    {
      "metadata": {
        "id": "Tu6N3dkYYBEq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "ratio_1 = np.sum(y_test.values) / len(y_test)\n",
        "print(\"The ratio of 'true' labels is  : {}\".format(ratio_1))\n",
        "print(\"This means a model predicting constants will have an accuracy of {}\".format(1 - ratio_1))\n",
        "\n",
        "labels = np.unique(y_train)\n",
        "class_weights = class_weight.compute_class_weight('balanced', labels, y_train)\n",
        "class_weights = dict(zip(labels, class_weights))\n",
        "weights_train = class_weight.compute_sample_weight(class_weights, y_train)\n",
        "weights_valid = class_weight.compute_sample_weight(class_weights, y_valid)\n",
        "\n",
        "# We have 2 years of data of each customer. If they weren't a customer for 2 years,\n",
        "# they start in status 'churned' before actually being a customer.\n",
        "# These datapoints don't exist when running in real life, but we need them to get fixed length timeseries\n",
        "# So we correct it by setting the weight to 0\n",
        "weights_train[train_init_churn == 1] = 0\n",
        "weights_valid[valid_init_churn == 1] = 0\n",
        "\n",
        "# Reshape weights for use in the neural net.\n",
        "weights_train_t = weights_train.reshape(y_train_t.shape[0], -1)\n",
        "weights_valid_t = weights_valid.reshape(y_valid_t.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5heO8ICW88G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the model architecture"
      ]
    },
    {
      "metadata": {
        "id": "XuL_hgvu9VWE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As our first network, we'll use an arbitrary LSTM model.\n",
        "\n",
        "The goal is to make you feel slightly uncomfortable. That's the best way to learn. If you're hungry for more after the MeetUp, this blog is a good place to start: [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras ](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)"
      ]
    },
    {
      "metadata": {
        "id": "XuDneUi6FoKj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def version_1():\n",
        "  \n",
        "  # Keras has two ways to define network structure:\n",
        "  # - incrementally through the sequential API\n",
        "  # - functionally, through the functional API\n",
        "  model = Sequential()\n",
        "\n",
        "  # Layer 1 - Standard dense layer.\n",
        "  model.add(\n",
        "        Dense(\n",
        "            128,  # The output dimension of the first layer.\n",
        "            # Standard activation function for neural nets. \n",
        "            # Read more here: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
        "            activation='relu',  \n",
        "            input_shape=(X_train_t.shape[1], X_train_t.shape[2])\n",
        "        )\n",
        "  )\n",
        "  \n",
        "  # Layer 2 - Second dense layer.\n",
        "  model.add(Dense(64))\n",
        "  \n",
        "  # Layer 3 - LSTM layers should always be preceded by a dense layer.\n",
        "  # Dropout is one way of dealing with overfitting in neural networks. It effectively sets a random fraction of inputs to 0.\n",
        "  # If you want to stack multiple recurrent layers, be sure to have `return_sequences` set to True.\n",
        "  model.add(\n",
        "      LSTM(128, return_sequences=True, activation='relu', dropout=0.2))\n",
        "  \n",
        "  # Layer 4 - The last recurrent sequence layer should be compressed with a TimeDistributed layer.\n",
        "  # If you don't want to use an LSTM, don't use a TimeDistributed layer.\n",
        "  model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
        "  \n",
        "  # Selecting the optimizer for the learning procedure.\n",
        "  # Feel free to tweak these values.\n",
        "  optimizer = keras.optimizers.Adam(\n",
        "      lr=5e-1, \n",
        "      beta_1=0.9, \n",
        "      beta_2=0.999, \n",
        "      epsilon=1e-08, \n",
        "      decay=0.0\n",
        "  )\n",
        "  \n",
        "  # Finally compile the model with a loss function and some metrics.\n",
        "  model.compile(\n",
        "      loss=losses.binary_crossentropy, \n",
        "      optimizer='adam', \n",
        "      metrics=['accuracy'], \n",
        "      sample_weight_mode=\"temporal\", \n",
        "      weighted_metrics=['accuracy']\n",
        "  )\n",
        "  \n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "baseline_model = version_1()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZgRPjkMjT_j6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiling and summarising the model is a helpful wait to show your model architecture. Compare the number of parameters to the total number of training points in the data. We need to ensure our model does not overfit by configuring the network properly. The dropout only partially accounts for overfitting."
      ]
    },
    {
      "metadata": {
        "id": "u0H1PFlLXF8I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Configure and run the network\n",
        "Now to configure running our network:"
      ]
    },
    {
      "metadata": {
        "id": "2yv9HLLZYBFY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def test(model):\n",
        "\n",
        "    # Reduce the learning rate automatically if model performance stagnates / overfitting occurs.\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='loss', \n",
        "        factor=0.3, \n",
        "        patience=10, \n",
        "        min_lr=1e-4,\n",
        "        verbose=1\n",
        "    )\n",
        "  \n",
        "    # Stop the learning procedure before the number of epochs is reached \n",
        "    # if performance on the validation set degrades.\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        min_delta=0, \n",
        "        patience=21, \n",
        "        verbose=0, \n",
        "        mode='auto'\n",
        "    )\n",
        "    \n",
        "    fit = model.fit(\n",
        "        X_train_t, y_train_t, epochs=1000,\n",
        "        validation_data=(X_valid_t, y_valid_t, weights_valid_t),\n",
        "        verbose=2, \n",
        "        sample_weight=weights_train_t,  # Relative importance weighting based on class imbalance\n",
        "        callbacks=[\n",
        "            reduce_lr, early_stopping],\n",
        "        batch_size=128\n",
        "        \n",
        "    )\n",
        "    \n",
        "    return fit\n",
        "\n",
        "baseline_fit = test(baseline_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugr2xczmT3MY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Do you understand the output of the learning algorithm?"
      ]
    },
    {
      "metadata": {
        "id": "sC9sBWNqYBFc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `reduce_lr` and `early_stopping` should collaborate well, in case the initial learning rate is way too high, it should have the possibility to be reduced enough before the early stopping kicks in."
      ]
    },
    {
      "metadata": {
        "id": "GChqHE17YBFd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Nr used epochs:      {}\".format(len(baseline_fit.history['loss'])))\n",
        "print(\"Train      loss: {}\".format(baseline_fit.history['loss'][-1]))\n",
        "print(\"Validation loss: {}\".format(baseline_fit.history['val_loss'][-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ntHxkJaDYJ4B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n"
      ]
    },
    {
      "metadata": {
        "id": "Ldvl-0lBYBFm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "summary_print(\n",
        "    y_train_t.reshape(-1),\n",
        "    baseline_fit.model.predict(X_train_t).reshape(-1),\n",
        "    'Train'\n",
        ")\n",
        "\n",
        "y_valid_hat = baseline_fit.model.predict(X_valid_t).reshape(-1)\n",
        "model_summary(y_valid_t.reshape(-1), y_valid_hat, 'Validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ct5OsmrLbZwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Is the network learning anything? Are you doing better than the previous baseline? If not, what could be going wrong? How could you improve the model?"
      ]
    },
    {
      "metadata": {
        "id": "KZZ2Z5YmYi8R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise:** Play with the network architecture and parameter settings and see what happens.\n",
        "\n",
        "Do you need to simplify? Add more layers? Are you overfitting? \n",
        "\n",
        "Try and build the best model possible."
      ]
    },
    {
      "metadata": {
        "id": "R5tF8TaO2qBV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## You've reached the end of the first part.\n",
        "\n",
        "The next section covers parameter search over network architecture and challenges your skills even more.\n",
        "\n",
        "You can skip the section and move to the test set verification.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "w8QuMnX9YBFv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7 - Advanced: Hyper parameter search\n",
        "Go directly to section 8 if you don't want to spend time on parameter searching.\n",
        "\n",
        "The previous network layout was quite arbitrary. We will now tune some characteristics of the layout.\n",
        "\n",
        "This could be done manually, where we the scientist would be executing some (randomized) gradient descent.\n",
        "For reproducibility we will however use the `hyperopt` package (https://github.com/hyperopt/hyperopt)\n"
      ]
    },
    {
      "metadata": {
        "id": "S9QXQAcJDLF8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the search candidate creation"
      ]
    },
    {
      "metadata": {
        "id": "8kGz1cUvYBFw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def create_model(depth, width, dropout=0.5, with_dense=True):\n",
        "    model = Sequential()\n",
        "    \n",
        "    if with_dense:\n",
        "        model.add(\n",
        "            Dense(\n",
        "                width, \n",
        "                activation='relu', \n",
        "                input_shape=(X_train_t.shape[1], X_train_t.shape[2])\n",
        "            )\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "        model.add(\n",
        "            LSTM(\n",
        "                width, \n",
        "                return_sequences=True, \n",
        "                activation='relu', \n",
        "                input_shape=(X_train_t.shape[1], X_train_t.shape[2])\n",
        "            )\n",
        "        )\n",
        "        \n",
        "    for x in range(depth-1):\n",
        "        model.add(\n",
        "            LSTM(\n",
        "                width, \n",
        "                return_sequences=True, \n",
        "                activation='relu', \n",
        "                dropout=dropout\n",
        "            )\n",
        "        )\n",
        "        \n",
        "    model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(\n",
        "        lr=5e-1, \n",
        "        beta_1=0.9, \n",
        "        beta_2=0.999, \n",
        "        epsilon=1e-08, \n",
        "        decay=0.0\n",
        "    )\n",
        "    \n",
        "    model.compile(\n",
        "        loss=losses.binary_crossentropy, \n",
        "        optimizer='adam', \n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    model.summary()\n",
        "    return model, \"depth{}_width{}_dropout{}\".format(depth, width, dropout)\n",
        "\n",
        "def fit_model(model, learning_params, name):\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='loss', \n",
        "        cooldown=learning_params['lr_plateau_cooldown'], \n",
        "        factor=learning_params['lr_plateau_factor'], \n",
        "        patience=learning_params['lr_plateau_patience'], \n",
        "        min_lr=learning_params['lr_minimum'],\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    label = \"{}_{}\".format(name, datetime.datetime.now().__str__())\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        min_delta=learning_params['stop_delta'],\n",
        "        patience=learning_params['stop_patience'], \n",
        "        verbose=0, \n",
        "        mode='auto'\n",
        "    )\n",
        "    \n",
        "    fit = model.fit(\n",
        "        X_train_t, y_train_t, epochs=learning_params['epochs_max'],\n",
        "        validation_data=(X_valid_t, y_valid_t),\n",
        "        verbose=2, \n",
        "        callbacks=[reduce_lr, early_stopping],\n",
        "        batch_size= 64\n",
        "    )\n",
        "    \n",
        "    return fit, {'label': label,\n",
        "                       'val_acc': fit.history['val_acc'][-1],\n",
        "                       'val_loss': fit.history['val_loss'][-1],\n",
        "                       'acc': fit.history['acc'][-1],\n",
        "                       'loss': fit.history['loss'][-1],\n",
        "                      }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qA4K3X615Nug",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parameter grid\n",
        "For doing a parameter search, we need to define a parameter grid."
      ]
    },
    {
      "metadata": {
        "id": "OemZZGb4YBFy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "learningrates1 = {\n",
        "'lr_intial' : 1e-2, # higher than the default\n",
        "'lr_decay' : 0.01, # for adam it is 1 / (1 + decay * t) , thus with decay 0.001 and at t==1000, lr is divided by 2\n",
        "                # note that the effect of this decay is not visible in tensorboard\n",
        "'lr_plateau_factor' : 0.7, # if no convergence (possibly by too high lr), we boost the lr decay\n",
        "'lr_plateau_patience' : 4, # nr of consequetive epochs without improvement before we boost the lr decay\n",
        "'lr_plateau_cooldown' : 10, # first do this nr of iterations at new lr before detecting plateau\n",
        "'lr_minimum' : 1e-6, # the minimum lr too which we decay (for plateau detection)\n",
        "\n",
        "'stop_patience' : 30, # if no extra improvement after this nr of steps , we terminate learning\n",
        "'stop_delta' : 0.0001, # the epsilon, changes below this threshold are 'no improvement'\n",
        "'epochs_max' : 1000 # limit the total nr op epochs, very high, we will stop based on plateau\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTTlNb5OYBF0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Some boiler plate for performing the search over the parameter space.\n",
        "import pickle\n",
        "import time\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, STATUS_FAIL, space_eval\n",
        "def find_result(trials, best):\n",
        "    \"\"\" \n",
        "    In a Trials object, finds the result that belongs to the the provided 'best' arguments.\n",
        "    \"\"\"\n",
        "    array_formed = dict([(k,[v]) for k,v in best.items()])\n",
        "    matched_runs = [x for x in trials.trials if x['misc']['vals']==array_formed]\n",
        "    return matched_runs[0]['result']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9B8y-GZs_PZ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Running the search\n",
        "Let's run the search. **Warning**: this takes a long time, so restrict the search space if you want results quickly."
      ]
    },
    {
      "metadata": {
        "id": "Cq_sK2PVYBF3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def experiment(args):\n",
        "    model, name = create_model(\n",
        "            depth = int(args['depth']), \n",
        "            width=int(args['width']), \n",
        "            dropout=args['dropout'],\n",
        "            with_dense=args['dense'])\n",
        "    fit, results = fit_model(model, learningrates1, 'widthsearch_'+name)\n",
        "    return name, fit, results\n",
        "        \n",
        "\n",
        "def objective(args):\n",
        "    print(' New trial '.center(60, '='))\n",
        "    print(args)\n",
        "    _, _, results = experiment(args)\n",
        "    return{\n",
        "        'status': STATUS_OK,\n",
        "        'loss':results['loss'],\n",
        "        'true_loss' : results['val_loss'],\n",
        "        # -- other metrics\n",
        "        'acc' : results['acc'],\n",
        "        'val_acc': results['val_acc'],\n",
        "        # -- diagnostics\n",
        "        'eval_time': time.time(),\n",
        "        'label': results['label']\n",
        "    }\n",
        "  \n",
        "  \n",
        "trials = Trials()\n",
        "space = {\n",
        "            'type' : 'lstms',\n",
        "            'width': hp.quniform('width', low=10, high=400, q=1), # note that low should be >=q to not get 0 (and therefore failed run)\n",
        "            'depth' : hp.quniform('depth', low=1, high=5, q=1), # note that low should be >=q to not get 0 (and therefore failed run)\n",
        "            'dropout' : hp.uniform('dropout', low=0.1, high=0.7),\n",
        "            'dense' : hp.choice('with_dense', [True, False])\n",
        "        }\n",
        "\n",
        "best = fmin(\n",
        "    objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=10,\n",
        "    trials=trials)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUZ0nwZWYBF-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Best choice for random variables:\")\n",
        "print(best) # the best choice,\n",
        "print(\"Complete set of arguments for best run:\")\n",
        "best_args = space_eval(space, best)\n",
        "print(best_args) # the full set of params for the best choice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9fEapOJYBF_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's get all information of the run according to the best run.  \n",
        "Note that this includes the label as it can be found in TensorBoard"
      ]
    },
    {
      "metadata": {
        "id": "Dv2BVOYNYBGA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "best_run = find_result(trials, best)\n",
        "best_run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1JmoxtOYBGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Final fit\n",
        "Given the hyper parameter search, we now know the best layout."
      ]
    },
    {
      "metadata": {
        "id": "HMfzShKpYBGG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The conclusion is that run with the following arguments was best:\")\n",
        "print(best_args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eE4WCK_UYBGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll train it again on the dataset to actually get the model instance, and subsequently verify it's performance"
      ]
    },
    {
      "metadata": {
        "id": "OZ6bP-wQYBGL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "best_name, best_fit, results = experiment(best_args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KfU5zWLuYBGN",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Nr used epochs:      {}\".format(len(best_fit.history['loss'])))\n",
        "print(\"Train      loss: {}\".format(best_fit.history['loss'][-1]))\n",
        "print(\"Validation loss: {}\".format(best_fit.history['val_loss'][-1]))\\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "23GxF3TwYBGP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "summary_print(y_train_t.reshape(-1),\n",
        "             best_fit.model.predict(X_train_t).reshape(-1),\n",
        "             'Train')\n",
        "y_valid_true = y_valid_t.reshape(-1)\n",
        "y_valid_hat = best_fit.model.predict(X_valid_t).reshape(-1)\n",
        "model_summary(y_valid_true, y_valid_hat, 'Validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ruIKCSNYBGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 8 - Final score on the test set\n",
        "When you are done tuning and selection, it is time to evaluate the performance on a hold out set.\n",
        "\n",
        "> Here we multiply the `model_score` by -1 to get the score comparable to the previous cross validations  \n",
        "> Note that the holdout test score will very likely be worse than the cv test score. One reason is that all meta params were selected to optimize that test score."
      ]
    },
    {
      "metadata": {
        "id": "J9BIS92VYBGV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "ugVJZ-YbYBGW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_test_true=y_test_t.reshape(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfz13bGdYBGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Quality of business rule\n"
      ]
    },
    {
      "metadata": {
        "id": "GkJSianZYBGY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_test_hat=np.repeat(0, len(y_test_true))\n",
        "model_summary(y_test_true, y_test_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UWlIuC5sNLsb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Quality of logistic regression"
      ]
    },
    {
      "metadata": {
        "id": "FbK44GitNLFn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Do test set predictions\n",
        "model_summary(y_valid, y_pred_proba_val_lr[:,1], 'Validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vba1fq2QNUfq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Quality of recurrent neural network"
      ]
    },
    {
      "metadata": {
        "id": "wZgUh3I-YBGb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_test_hat = baseline_fit.model.predict(X_test_p, verbose=2).reshape(-1)\n",
        "model_summary(y_test_true, y_test_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YnmU8-8DYBGd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Quality of tuned recurrent neural network"
      ]
    },
    {
      "metadata": {
        "id": "QCXpRC62YBGh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_test_hat = best_fit.model.predict(X_test_p, verbose=2).reshape(-1)\n",
        "model_summary(y_test_true, y_test_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DN1PtugINbpy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You've made it to the end.\n",
        "\n",
        "If you still want more to do, try to improve on your last model. "
      ]
    },
    {
      "metadata": {
        "id": "_6cDP2jMY46E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feedback questionary\n",
        "Thank you for joining our event. We would be grateful to hear about your experience:\n",
        "\n",
        "https://www.surveymonkey.com/r/KYSLSHQ"
      ]
    },
    {
      "metadata": {
        "id": "Cv-K-ibzGIz0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Further reading\n",
        "If you liked this hands-on session, but would like to have a deeper understanding on certain topics, have a look at the references here.\n",
        "\n",
        "- [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras ](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) - a more in depth explanation of how to build your own LSTM for time series prediction.\n",
        "\n",
        "- [Gentle introduction to the ADAM optimization algorithm](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) - for learning more on how to tweak parameters.\n",
        "\n",
        "- [Algorithmic marketing for AI for marketing operations\n",
        "](https://algorithmicweb.wordpress.com/) - very good overview book aimed at marketeers, covering many topics, including churn prediction.\n",
        "\n",
        "- [Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/) - There's a neural network for everything. This is an overview showing the different types of networks.\n",
        "\n",
        "- [Time to event modelling with Neural Networks](https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/) - Survival Analysis with neural networks? This blog shows an example of how to make it work.\n",
        "\n"
      ]
    }
  ]
}